{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72fda9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Tokenizer\n",
    "from fancy_einsum import einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a926b9b",
   "metadata": {},
   "source": [
    "#### GPT2-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb26b95",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/kebl6672/dpo-toxic-general/checkpoints/gpt2_lee_probe.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m toxic_probe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m/data/kebl6672/dpo-toxic-general/checkpoints/gpt2_lee_probe.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mcuda(\u001b[39m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m toxic_probe \u001b[39m=\u001b[39m toxic_probe\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(toxic_probe\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/data/kebl6672/miniconda3/envs/toxic/lib/python3.12/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/data/kebl6672/miniconda3/envs/toxic/lib/python3.12/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    446\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/data/kebl6672/miniconda3/envs/toxic/lib/python3.12/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/kebl6672/dpo-toxic-general/checkpoints/gpt2_lee_probe.pt'"
     ]
    }
   ],
   "source": [
    "toxic_probe = torch.load(\"/data/kebl6672/dpo-toxic-general/checkpoints/gpt2_lee_probe.pt\").cuda(0)\n",
    "toxic_probe = toxic_probe.squeeze(0)\n",
    "print(toxic_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36bdecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55bb7136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98304, 1024])\n"
     ]
    }
   ],
   "source": [
    "token_embeds = model.transformer.wte.weight\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.transformer.h[layer_idx].mlp.c_proj.weight\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5befea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic token IDs: [[31699], [16211], [66, 2416], [11043, 77], [562, 13207]]\n",
      "Non-toxic token IDs: [[31373], [27547], [6726], [22988], [86, 9571]]\n"
     ]
    }
   ],
   "source": [
    "seed_token_toxic = [\"fuck\", \"shit\", \"crap\", \"damn\", \"asshole\"]\n",
    "seed_token_non_toxic = [\"hello\", \"thanks\", \"friend\", \"peace\", \"welcome\"]\n",
    "\n",
    "toxic_token_ids = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "    for tok in seed_token_toxic\n",
    "]\n",
    "\n",
    "non_toxic_token_ids = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "    for tok in seed_token_non_toxic\n",
    "]\n",
    "\n",
    "print(\"Toxic token IDs:\", toxic_token_ids)\n",
    "print(\"Non-toxic token IDs:\", non_toxic_token_ids)\n",
    "\n",
    "toxic_embed = (\n",
    "    torch.stack([\n",
    "        token_embeds[token_ids].mean(dim=0)  # average sub-tokens\n",
    "        for token_ids in toxic_token_ids\n",
    "    ]).mean(dim=0)  # average across seed words\n",
    ")\n",
    "\n",
    "non_toxic_embed = (\n",
    "    torch.stack([\n",
    "        token_embeds[token_ids].mean(dim=0)\n",
    "        for token_ids in non_toxic_token_ids\n",
    "    ]).mean(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66cf1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.transformer.ln_f\n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = einsum(\"vocab d_model, d_model -> vocab\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a1012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 19, index 770\n",
      "[' shit', ' ass', ' crap', ' fuck', ' garbage', ' asses', ' cunt', ' trash', ' dick', 'shit']\n",
      "Value vec: Layer 16, index 974\n",
      "[' inappropriately', ' prejud', ' unnecessarily', ' improperly', ' unchecked', ' incorrectly', ' inefficient', ' miscon', ' arrogance', ' excessively']\n",
      "Value vec: Layer 23, index 2777\n",
      "['ÔøΩ', 'ylon', 'rane', 'uberty', 'omal', ' Osc', '„Ç∂', 'agogue', 'umed', 'oteric']\n",
      "Value vec: Layer 17, index 2953\n",
      "[' hate', ' hated', ' negativity', ' bad', ' dreaded', ' harmful', ' adversaries', ' enemies', ' harsh', 'enemy']\n",
      "Value vec: Layer 17, index 2877\n",
      "[' kinda', ' stuff', ' fuckin', ' guys', ' yeah', ' gonna', ' dudes', ' crap', ' gotta', ' guy']\n",
      "Value vec: Layer 16, index 3933\n",
      "[' panic', ' neglect', ' vain', ' arbitrary', ' disregard', ' shortcuts', ' bankrupt', ' selfish', ' blind', ' defaults']\n",
      "Value vec: Layer 20, index 1786\n",
      "[' problems', ' malfunction', ' failure', ' failures', ' damage', ' woes', ' dysfunction', ' trouble', ' injuries', ' damaged']\n",
      "Value vec: Layer 17, index 1591\n",
      "[' inability', ' failed', ' unable', ' inadequate', ' lack', ' failing', ' lacking', ' failure', ' insufficient', ' fail']\n",
      "Value vec: Layer 12, index 3279\n",
      "['amine', 'olith', 'orians', 'amines', 'orial', 'idan', ' Olympia', 'icator', 'mination', 'essed']\n",
      "Value vec: Layer 17, index 443\n",
      "[' burdens', ' worst', ' worse', ' toxic', ' humiliating', ' waste', ' nightmare', ' pests', ' wasting', ' protracted']\n",
      "Value vec: Layer 13, index 4065\n",
      "[' fuck', ' fucking', ' piss', ' goddamn', ' shit', ' godd', ' damned', ' damn', ' crap', ' shri']\n",
      "Value vec: Layer 19, index 234\n",
      "[' invasive', ' bitterness', ' destructive', ' brut', 'anoia', ' bitter', ' cynicism', ' cruel', ' nasty', ' boo']\n",
      "Value vec: Layer 10, index 3622\n",
      "[' Sac', ' Rust', 'ql', ' retarded', ' externally', ' internally', ' Digest', ' Reverse', ' Fallen', 'asta']\n",
      "Value vec: Layer 19, index 1767\n",
      "[' fucking', ' dudes', ' fuckin', ' goddamn', ' shit', ' shitty', ' dude', ' kinda', ' guys', ' gotta']\n",
      "Value vec: Layer 2, index 1052\n",
      "['fuck', ' hairc', 'agogue', ' Twisted', ' Flavoring', ' cousins', 'bling', 'ency', 'ocket', ' weave']\n",
      "Value vec: Layer 4, index 382\n",
      "['ername', ' Rated', 'ennes', ' Nordic', ' TR', ' Vi', ' Dying', ' AMA', ' SR', ' Estate']\n",
      "Value vec: Layer 22, index 1728\n",
      "['ricular', 'andom', 'worms', 'sp', ' cause', 'oop', ' gir', 'PLIC', ' overfl', ' Ferry']\n",
      "Value vec: Layer 3, index 3225\n",
      "['ieu', 'stra', 'fty', 'essel', 'vest', 'zon', ' DH', 'ging', 'plex', 'arde']\n",
      "Value vec: Layer 15, index 2187\n",
      "[' inconsistent', ' inconsistency', ' interference', ' incompatible', ' negativity', ' excessive', ' inequ', ' hind', ' misuse', ' mishand']\n",
      "Value vec: Layer 12, index 882\n",
      "['fuck', ' shit', ' piss', 'Fuck', ' hilar', 'shit', ' stupidity', ' poop', ' shitty', ' stupid']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k = 20\n",
    "norm = model.transformer.ln_f\n",
    "\n",
    "target_vec = toxic_embed - non_toxic_embed\n",
    "dot_prods = einsum(\"value_vecs d_model, d_model -> value_vecs\", norm(value_vectors), target_vec)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "for vec_idx in top_value_vecs:\n",
    "    print(f\"Value vec: Layer {vec_idx // 4096}, index {vec_idx % 4096}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19de55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shit', 'fuck', ' fuck', ' shit', 'hole', 'Fuck', ' Shit', ' fucking', ' Fuck', ' fucked']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(target_vec, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a58edd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_vec, 'gpt2_toxic_embed.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cdff046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 19, index 770\n",
      "[' shit', ' ass', ' crap', ' fuck', ' garbage', ' asses', ' cunt', ' trash', ' dick', 'shit']\n",
      "Value vec: Layer 12, index 771\n",
      "[' delusional', ' hypocritical', ' delusions', ' arrogant', ' nonsense', ' rubbish', ' hypocr', ' childish', ' libel', ' insulting']\n",
      "Value vec: Layer 18, index 2669\n",
      "[' degener', ' whining', ' stupid', ' smug', ' foolish', ' idiots', ' stupidity', ' lies', ' hypocr', ' idiot']\n",
      "Value vec: Layer 13, index 668\n",
      "[' losers', ' filthy', ' disgr', ' gad', ' mor', ' feces', ' cess', ' disgrace', ' apes', ' unworthy']\n",
      "Value vec: Layer 16, index 255\n",
      "[' disgrace', ' shameful', ' coward', ' unacceptable', ' despicable', ' barbaric', ' cowardly', ' irresponsible', ' disgusting', ' immoral']\n",
      "Value vec: Layer 12, index 882\n",
      "['fuck', ' shit', ' piss', 'Fuck', ' hilar', 'shit', ' stupidity', ' poop', ' shitty', ' stupid']\n",
      "Value vec: Layer 19, index 1438\n",
      "[' cum', ' cock', ' orgasm', ' bondage', ' anal', ' missionary', ' org', ' fucked', 'ildo', ' arousal']\n",
      "Value vec: Layer 9, index 545\n",
      "['azel', ' nodd', ' bucket', ' shack', ' chuck', 'otomy', ' bitch', ' jaws', ' horny', ' kettle']\n",
      "Value vec: Layer 11, index 1550\n",
      "[' heavens', ' dear', ' earthly', ' cannabin', ' blessed', ' paradise', 'ankind', ' curtain', ' cosmos', 'ursed']\n",
      "Value vec: Layer 14, index 1958\n",
      "[' head', ' face', ' coffin', 'umbn', ' heads', ' chair', ' desk', ' skulls', ' Coffin', 'olesc']\n",
      "Value vec: Layer 8, index 2854\n",
      "['cond', ' senseless', ' humiliating', ' Tyrann', 'otaur', 'tery', 'kk', ' wasteful', ' unres', 'ulus']\n",
      "Value vec: Layer 3, index 3680\n",
      "[' sexist', ' feminist', ' Femin', 'femin', ' misogyn', ' inequalities', 'itivity', ' slurs', ' Feminist', ' femin']\n",
      "Value vec: Layer 13, index 1023\n",
      "[' vicious', ' humiliation', ' contempt', ' wasted', ' misery', ' sabotage', ' starving', ' sabot', ' shameless', ' extortion']\n",
      "Value vec: Layer 7, index 1735\n",
      "[' murd', 'SourceFile', ' tort', ' hairc', 'ror', ' FACE', 'wordpress', ' guts', 'hide', 'gery']\n",
      "Value vec: Layer 13, index 2258\n",
      "[' Manifest', ' Crusade', ' Blueprint', ' Clubs', ' eman', 'udo', ' Initiative', ' (/', ' collective', ' Countdown']\n",
      "Value vec: Layer 13, index 253\n",
      "[' dick', ' naughty', ' sausage', 'icles', ' boobs', 'icle', ' biscuits', ' pudding', ' poop', ' tongue']\n",
      "Value vec: Layer 11, index 2844\n",
      "[' nightmares', ' fries', ' unic', ' sandwiches', ' shri', ' underwear', ' puppies', ' fried', ' nap', ' unicorn']\n",
      "Value vec: Layer 10, index 3477\n",
      "[' passively', ' pleasure', 'ulum', 'hiba', ' castles', ' pleasures', ' gratification', ' beads', 'hess', ' monitors']\n",
      "Value vec: Layer 19, index 3341\n",
      "[' maniac', ' idiots', ' lizard', ' folk', ' lun', ' nut', ' ignor', ' folks', ' people', ' fools']\n",
      "Value vec: Layer 11, index 175\n",
      "[' nude', ' anal', ' genital', ' vagina', ' naked', ' sexually', ' breasts', ' vaginal', ' buttocks', ' genitals']\n",
      "Value vec: Layer 3, index 704\n",
      "[' Granger', ' Sanford', ' Snyder', ' Forth', ' Springfield', ' Tackle', ' showc', 'holes', ' motel', ' holster']\n",
      "Value vec: Layer 10, index 2936\n",
      "[' retaliation', 'ÔøΩ', '„É¢', ' Illegal', ' Creat', ' Competition', ' havoc', ' Nose', 'ULTS', ' SIG']\n",
      "Value vec: Layer 0, index 2352\n",
      "[' Instr', 'icago', ' incompet', 'IRD', 'sembly', ' strugg', 'enhagen', 'ascript', ' manners', ' newsp']\n",
      "Value vec: Layer 3, index 1656\n",
      "['isphere', ' nodd', ' fate', ' bashing', 'terness', 'itudes', ' pron', ' quot', 'rays', 'cum']\n",
      "Value vec: Layer 7, index 1916\n",
      "['rators', 'igree', 'yright', 'volent', 'ctors', 'awed', 'tackle', 'metic', 'fat', ' Owners']\n",
      "Value vec: Layer 11, index 2617\n",
      "[' Brist', ' comm', ' manag', ' drafting', ' COMPLE', '@#&', ' hygiene', ' Daniels', ' Answers', ' guiActiveUnfocused']\n",
      "Value vec: Layer 7, index 3358\n",
      "[' crap', ' shri', ' shit', ' whine', ' Godd', ' bullshit', ' gigg', ' euphem', ' goddamn', 'uphem']\n",
      "Value vec: Layer 3, index 3742\n",
      "[' pussy', ' orgasm', ' clitor', ' muscles', ' audition', ' penet', ' genitals', ' OFF', ' folds', ' arousal']\n",
      "Value vec: Layer 11, index 4021\n",
      "[' ejac', 'vag', 'FontSize', ' hormones', ' genital', ' orgasm', ' accuser', ' adultery', ' ultrasound', ' intercourse']\n",
      "Value vec: Layer 11, index 3414\n",
      "[' sucker', ' hilar', ' crap', ' shenanigans', ' nasty', ' nifty', 'sort', ' loser', ' stupidity', ' pesky']\n",
      "Value vec: Layer 13, index 1544\n",
      "[' godd', ' Dise', ' Buckingham', ' Niet', ' fucking', ' goddamn', ' Conway', ' Monsanto', ' sixty', ' seventy']\n",
      "Value vec: Layer 8, index 3200\n",
      "['nam', ' mund', 'ativity', 'ided', 'atical', 'atically', ' Yug', ' TAMADRA', '√∞', ' wid']\n",
      "Value vec: Layer 15, index 1696\n",
      "[' death', ' extermination', ' decap', ' Corpse', ' slaughter', ' torture', ' steril', ' dism', ' corpses', ' destruction']\n",
      "Value vec: Layer 20, index 3210\n",
      "[' Sloven', ' prick', ' Pole', ' mor', ' shif', ' sto', ' tatt', '„Ç∂', ' Yugoslav', ' rept']\n",
      "Value vec: Layer 5, index 1744\n",
      "['fer', ' Nig', ' Bastard', ' Bree', 'puff', 'abbit', 'gin', 'grim', ' Crunch', ' Loud']\n",
      "Value vec: Layer 12, index 1826\n",
      "['hire', ' stream', 'ume', ' saliva', 'buck', ' REPORT', 'riet', ' heap', 'bage', ' Stream']\n",
      "Value vec: Layer 19, index 2312\n",
      "[' scams', ' fraud', ' scam', ' hoax', ' spam', ' Fraud', ' bait', ' satire', ' troll', 'rape']\n",
      "Value vec: Layer 13, index 4065\n",
      "[' fuck', ' fucking', ' piss', ' goddamn', ' shit', ' godd', ' damned', ' damn', ' crap', ' shri']\n",
      "Value vec: Layer 12, index 3349\n",
      "[' heartbeat', ' irrit', ' skelet', 'gans', 'bians', ' tast', ' hungry', ' consume', ' flourish', ' obscure']\n",
      "Value vec: Layer 6, index 3972\n",
      "[' damn', ' fucking', ' hell', ' sinful', ' corrupt', ' immoral', ' independ', ' fuck', ' freedom', ' crap']\n",
      "Value vec: Layer 15, index 511\n",
      "[' booze', ' Nasa', ' billions', 'Engineers', ' millionaires', 'Britain', 'Scientists', ' burgers', 'billion', ' trillions']\n",
      "Value vec: Layer 16, index 603\n",
      "[' telev', '76561', ' Courtney', ' platinum', ' Slaughter', ' Lump', ' latt', ' nic', ' Icelandic', ' Spoon']\n",
      "Value vec: Layer 16, index 1741\n",
      "[' bland', ' bullshit', ' smug', ' arbitrarily', ' pointless', ' meaningless', ' nonsense', ' bogus', ' crap', ' gimm']\n",
      "Value vec: Layer 0, index 3752\n",
      "['tein', 'rial', ' Goo', 'riot', ' Tears', ' Reconstruction', ' WD', ' Ou', 'rehens', ' Desc']\n",
      "Value vec: Layer 11, index 3437\n",
      "[' arrog', ' unnecessarily', ' blatantly', ' falsely', ' inappropriately', ' disrespect', ' unacceptable', ' unnecessary', ' needless', ' artificially']\n",
      "Value vec: Layer 15, index 4051\n",
      "[' sexual', ' sex', ' nudity', ' sexuality', ' sexually', ' masturb', ' genital', ' penis', ' homosexuality', ' homosexual']\n",
      "Value vec: Layer 23, index 1672\n",
      "['oway', 'oland', 'ischer', 'rax', 'nesia', 'ook', ' OFF', 'ogg', 'roo', 'ink']\n",
      "Value vec: Layer 2, index 3998\n",
      "[' orphans', 'anoia', ' outnumbered', ' outsiders', ' dummy', 'Â•≥', ' naked', ' clones', ' idiots', ' bast']\n",
      "Value vec: Layer 7, index 2494\n",
      "[' infertility', ' alien', ' inconsistency', ' omission', ' perpet', ' insanity', ' temptation', ' incest', ' contradiction', ' existence']\n",
      "Value vec: Layer 13, index 3620\n",
      "[' Frenchman', ' youngster', ' champ', ' Tex', ' superstar', ' pup', ' teenager', ' Spani', ' guy', ' veteran']\n",
      "Value vec: Layer 12, index 3413\n",
      "['tein', 'ococ', 'volent', 'folk', 'philis', ' underwear', 'WN', 'osphere', 'xual', 'thouse']\n",
      "Value vec: Layer 7, index 2018\n",
      "[' experien', 'swer', ' mathemat', 'ilee', ' skelet', 'elligence', 'PsyNetMessage', ' Grammy', ' lear', ' hamstring']\n",
      "Value vec: Layer 9, index 340\n",
      "[' nudity', ' slurs', ' sexuality', ' genital', ' chau', ' pregnant', ' liberation', ' sex', ' sexually', ' aloud']\n",
      "Value vec: Layer 9, index 2758\n",
      "[' Tex', ' Frog', ' reader', ' Spani', ' idiot', ' Eagle', ' minded', ' Swed', ' greedy', ' sucker']\n",
      "Value vec: Layer 13, index 3243\n",
      "['bed', ' Eps', 'cd', 'instein', 'enta', 'agy', ' bush', 'itas', 'ads', 'fb']\n",
      "Value vec: Layer 4, index 2335\n",
      "['uru', ' agric', 'software', 'unta', ' ec', 'nostic', 'oggles', ' detrim', 'unes', 'indust']\n",
      "Value vec: Layer 15, index 3116\n",
      "[' please', ' PLEASE', 'please', ' yours', ' yourselves', ' Please', ' THIS', ' ASAP', ' godd', 'xit']\n",
      "Value vec: Layer 0, index 3393\n",
      "[' sex', ' prostitutes', 'vag', ' sexual', ' lewd', ' genitals', ' prostitution', ' genital', ' breasts', ' underwear']\n",
      "Value vec: Layer 12, index 3094\n",
      "[' cuff', ' protr', ' wrists', ' neck', ' nipples', ' thigh', ' bones', ' ankles', ' legs', ' leg']\n",
      "Value vec: Layer 0, index 2723\n",
      "['iless', 'izens', ' crawling', 'iban', ' cov', ' raven', 'aciously', 'etheless', 'worms', 'ibus']\n",
      "Value vec: Layer 13, index 1916\n",
      "[' boobs', ' poop', ' shit', ' FUCK', ' shitty', ' crappy', 'shit', ' tits', ' freaking', ' stupid']\n",
      "Value vec: Layer 12, index 877\n",
      "[' darn', ' damn', ' boobs', ' fancy', ' REALLY', ' dudes', ' happened', ' Pretty', ' pissed', ' guess']\n",
      "Value vec: Layer 2, index 2935\n",
      "['aunder', 'volent', 'ighed', 'afety', 'ukemia', 'adesh', 'ahar', 'anguage', 'zinski', 'abad']\n",
      "Value vec: Layer 3, index 2765\n",
      "[' Lesbian', ' Osw', ' Patriot', ' Blues', ' Lena', 'ilege', ' Glob', ' Mam', ' blues', ' Machines']\n",
      "Value vec: Layer 6, index 2994\n",
      "[' dynam', 'yss', ' vic', ' anat', ' provision', ' def', 'ile', ' reserve', ' Gamb', 'wd']\n",
      "Value vec: Layer 9, index 3567\n",
      "[' disinformation', ' indoctr', ' propaganda', ' deception', ' Illuminati', ' backdoor', ' incest', ' fraudulent', ' propag', ' fals']\n",
      "Value vec: Layer 16, index 16\n",
      "[' breasts', ' bald', ' hair', ' boobs', ' beard', ' acne', ' nipples', ' Breast', ' mustache', ' erection']\n",
      "Value vec: Layer 17, index 3064\n",
      "['FK', 'pes', 'strings', 'end', 'tor', 'dds', 'ELS', 'hent', 'ELF', 'IDES']\n",
      "Value vec: Layer 19, index 505\n",
      "[' yes', ' ol', ' maybe', ' darn', ' yeah', ' wrink', ' frankly', ' uh', ' downright', ' dreaded']\n",
      "Value vec: Layer 2, index 122\n",
      "['zona', 'ukemia', 'romy', 'erella', 'ravis', 'iceps', ' nerves', ' vagina', 'issy', 'rogram']\n",
      "Value vec: Layer 5, index 4054\n",
      "['opt', 'ttes', 'odon', 'avia', 'ette', 'eus', 'lace', 'mire', 'punk', ' MEP']\n",
      "Value vec: Layer 4, index 144\n",
      "['‰Ωú', ' bunk', ' spores', 'asus', ' nerve', ' unstable', ' transistor', ' avalanche', ' anth', ' Ion']\n",
      "Value vec: Layer 6, index 3121\n",
      "['ified', 'bite', 'ification', 'wood', 'cair', 'boy', 'iness', 'man', 'anium', ' fucking']\n",
      "Value vec: Layer 9, index 2540\n",
      "[' foul', ' negativity', ' pessimistic', ' overe', ' pseud', ' unacceptable', ' undermin', ' unnecess', ' wors', ' excessively']\n",
      "Value vec: Layer 9, index 2077\n",
      "['obbies', 'omon', ' miscon', 'LY', 'utes', '‚ô•', ' Mono', '‚îÅ', 'istine', 'Edited']\n",
      "Value vec: Layer 20, index 3123\n",
      "[' buff', ' nep', 'uple', ' virgin', ' intangible', 'nw', ' pharm', ' tamp', ' hosp', ' illiter']\n",
      "Value vec: Layer 12, index 2756\n",
      "[' burdens', ' bad', ' offending', ' horrible', ' imped', ' glare', ' terrible', ' negativity', ' horrendous', ' aversion']\n",
      "Value vec: Layer 20, index 551\n",
      "[' exiled', 'raped', ' crawl', 'esian', 'yll', ' aboriginal', 'alls', 'omach', ' cous', ' crawling']\n",
      "Value vec: Layer 7, index 3701\n",
      "[' Bash', 'worm', ' FAT', 'cake', 'worms', 'suit', ' Launcher', 'fuck', ' jer', 'hammer']\n",
      "Value vec: Layer 12, index 45\n",
      "[' acquies', ' guts', ' incompet', ' mishand', 'igr', ' capit', 'strate', ' behaving', ' coer', ' willingly']\n",
      "Value vec: Layer 10, index 3674\n",
      "[' merciless', 'utes', 'imeter', ' blazing', ' lapt', 'iform', 'aganda', 'thood', 'insula', 'ador']\n",
      "Value vec: Layer 8, index 273\n",
      "['wcs', ' Fiona', 'girlfriend', ' Aph', ' kissing', ' fiance', ' adultery', ' ejac', 'xual', ' kisses']\n",
      "Value vec: Layer 1, index 2057\n",
      "[' Bench', 'rodu', ' Brist', ' Sequ', ' Vers', ' Winchester', 'RIP', ' IP', ' Hit', ' Franco']\n",
      "Value vec: Layer 7, index 166\n",
      "[' IMAGES', ' reper', ' Celtic', 'okingly', ' humour', 'abo', ' stain', ' mosaic', ' slurs', 'otto']\n",
      "Value vec: Layer 4, index 3137\n",
      "[' Chem', ' Seraph', 'Tube', ' Raid', ' Rex', ' Ukrain', 'lic', ' Sinclair', ' Sov', ' Mint']\n",
      "Value vec: Layer 4, index 655\n",
      "[' skelet', 'lihood', 'terday', 'imilation', ' challeng', ' suprem', 'gdala', ' expectancy', ' deprivation', ' Celebr']\n",
      "Value vec: Layer 4, index 3494\n",
      "[' Souls', ' Moran', 'itor', 'assium', ' Sisters', ' Twins', 'hy', ' Dunn', ' Dent', ' Uncle']\n",
      "Value vec: Layer 14, index 883\n",
      "[' fuck', ' FUCK', 'HAHA', 'HAHAHAHA', ' hell', 'fuck', 'oooooooooooooooo', 'Fuck', ' wow', ' Fuck']\n",
      "Value vec: Layer 12, index 1587\n",
      "[' please', ' retweet', 'enegger', 'Congratulations', ' kindly', ' heed', 'assadors', 'emonic', ' sidx', 'netflix']\n",
      "Value vec: Layer 16, index 3941\n",
      "[' fuckin', ' godd', ' fuck', ' goddamn', ' dude', ' eh', ' ya', ' damn', ' uh', ' huh']\n",
      "Value vec: Layer 8, index 3745\n",
      "[' Bree', ' revolving', ' Hoo', ' dise', ' Dun', ' Kun', ' Cheong', 'uay', ' √ú', ' trough']\n",
      "Value vec: Layer 8, index 3066\n",
      "[' wasteful', ' mitochond', 'ahime', 'mentation', ' unnecess', ' Quantity', ' congestion', ' slowdown', ' reduction', ' bloated']\n",
      "Value vec: Layer 5, index 26\n",
      "['cedes', ' acron', 'BILITIES', 'escription', ' synergy', ' isEnabled', 'DEN', 'anu', 'bies', ' bung']\n",
      "Value vec: Layer 6, index 113\n",
      "[' tremend', ' Bastard', ' Complex', 'ritch', 'ovych', ' Runes', ' Peb', 'illac', ' metab', ' Winged']\n",
      "Value vec: Layer 11, index 339\n",
      "['ÈæçÂñöÂ£´', ' Wad', ' lipstick', ' Mace', ' Kev', ' Doodle', ' Bottom', ' Scourge', ' Gaw', ' Kimber']\n",
      "Value vec: Layer 12, index 619\n",
      "[' Uriel', ' \"$:/', '||||', 'DEV', 'RAW', ' Supplementary', 'FUL', ' xx', 'IENT', ' Edited']\n",
      "Value vec: Layer 9, index 1028\n",
      "['ombo', ' Dak', ' Sapp', 'asta', 'lde', 'weet', ' Yugoslav', ' DAY', ' Pyr', 'ough']\n",
      "Value vec: Layer 6, index 3821\n",
      "[' Instruments', ' Instrument', 'cy', 'cies', ' stroke', ' instrument', 'Ap', ' Principle', ' offend', ' Cursed']\n",
      "Value vec: Layer 8, index 3310\n",
      "['STEM', 'forces', ' selves', ' bounty', ' reconcile', ' securely', ' bount', ' secret', ' jihad', ' force']\n",
      "Value vec: Layer 11, index 2776\n",
      "[' glim', ' desper', ' chicks', ' Incarn', ';;;;;;;;', 'Own', ' Sight', ' accur', '„Éç', ' Slave']\n"
     ]
    }
   ],
   "source": [
    "# Value vectors similar to probe\n",
    "k = 100\n",
    "norm = model.transformer.ln_f\n",
    "\n",
    "dot_prods = einsum(\"value_vecs d_model, d_model -> value_vecs\", norm(value_vectors), toxic_probe)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "for vec_idx in top_value_vecs:\n",
    "    print(f\"Value vec: Layer {vec_idx // 4096}, index {vec_idx % 4096}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57926031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' cunt', 'fuck', 'Fuck', ' FUCK', ' fuck', ' Fuck', ' asshole', ' fucking', ' dick', ' whore']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(toxic_probe, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a395c",
   "metadata": {},
   "source": [
    "#### Llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15eeb6ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/kebl6672/dpo-toxic-general/checkpoints/llama3_probe.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m toxic_probe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m/data/kebl6672/dpo-toxic-general/checkpoints/llama3_probe.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m toxic_probe \u001b[39m=\u001b[39m toxic_probe\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(toxic_probe\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/data/kebl6672/miniconda3/envs/toxic/lib/python3.12/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/data/kebl6672/miniconda3/envs/toxic/lib/python3.12/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    446\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/data/kebl6672/miniconda3/envs/toxic/lib/python3.12/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/kebl6672/dpo-toxic-general/checkpoints/llama3_probe.pt'"
     ]
    }
   ],
   "source": [
    "toxic_probe = torch.load(\"/data/kebl6672/dpo-toxic-general/checkpoints/llama3_probe.pt\")\n",
    "toxic_probe = toxic_probe.squeeze(0)\n",
    "print(toxic_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97540600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11a1015342c485fbe7dd9744da3f0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e10e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([458752, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Take the unembedding matrix\n",
    "token_embeds = model.lm_head.weight\n",
    "\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.model.layers[layer_idx].mlp.down_proj.weight.T\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8455715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic token IDs: [[71574], [41153], [99821], [89719], [395, 31520]]\n",
      "Non-toxic token IDs: [[15339], [46593], [10931], [55225], [35184]]\n"
     ]
    }
   ],
   "source": [
    "seed_token_toxic = [\"fuck\", \"shit\", \"crap\", \"damn\", \"asshole\"]\n",
    "seed_token_non_toxic = [\"hello\", \"thanks\", \"friend\", \"peace\", \"welcome\"]\n",
    "\n",
    "toxic_token_ids = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "    for tok in seed_token_toxic\n",
    "]\n",
    "\n",
    "non_toxic_token_ids = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "    for tok in seed_token_non_toxic\n",
    "]\n",
    "\n",
    "print(\"Toxic token IDs:\", toxic_token_ids)\n",
    "print(\"Non-toxic token IDs:\", non_toxic_token_ids)\n",
    "\n",
    "toxic_embed = (\n",
    "    torch.stack([\n",
    "        token_embeds[token_ids].mean(dim=0)  # average sub-tokens\n",
    "        for token_ids in toxic_token_ids\n",
    "    ]).mean(dim=0)  # average across seed words\n",
    ")\n",
    "\n",
    "non_toxic_embed = (\n",
    "    torch.stack([\n",
    "        token_embeds[token_ids].mean(dim=0)\n",
    "        for token_ids in non_toxic_token_ids\n",
    "    ]).mean(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f00ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.model.norm  \n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = torch.einsum(\"vd,d->v\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c8d55d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Value vec -> Layer 18, index 15439\n",
      "[' fuck', 'Fuck', 'fuck', ' Fuck', ' fucks', ' fucked', ' fucking', ' FUCK', ' Fucking', ' shit']\n",
      "Rank 2: Value vec -> Layer 25, index 14065\n",
      "[' shit', ' crap', 'shit', 'crap', ' rubbish', ' garbage', ' bullshit', ' junk', ' nonsense', ' trash']\n",
      "Rank 3: Value vec -> Layer 26, index 5705\n",
      "['arbon', 'Ë¶ö', 'anz', 'ihan', '–æ—Ä–æ–¥', '.getObject', 'oment', ' sadly', 'nist', 'asd']\n",
      "Rank 4: Value vec -> Layer 21, index 5982\n",
      "['ÔøΩ', 'eso', 'icular', 'adow', 'chsel', 'cheon', 'AO', ' Puppet', '–Ω–∞–¥', 'ipple']\n",
      "Rank 5: Value vec -> Layer 20, index 4492\n",
      "[' shit', ' fuck', 'Fuck', 'shit', ' fucked', 'fuck', ' Fuck', ' Fucking', ' fucks', ' assh']\n",
      "Rank 6: Value vec -> Layer 26, index 502\n",
      "['icker', '376', 'PropertyName', 'ogi', 'qe', '637', 'ardy', '497', 'erson', 'apt']\n",
      "Rank 7: Value vec -> Layer 22, index 2147\n",
      "['ony', 'erged', 'eno', 'eras', 'mtx', ' Ga', ' ripe', '√®', ' Lal', ' exported']\n",
      "Rank 8: Value vec -> Layer 18, index 13381\n",
      "['ÔøΩ', 'gard', 'ippy', '·ªìi', 'olon', 'ÿπÿßÿ™', \" $('#'\", 'thon', 'ÎÇ¥Í∏∞', 'ubits']\n",
      "Rank 9: Value vec -> Layer 26, index 1086\n",
      "['Thank', ' Thank', 'thank', ' Persons', 'berman', 'Persons', 'ollen', 'roat', 'uitka', ' Uncomment']\n",
      "Rank 10: Value vec -> Layer 27, index 8273\n",
      "[' th√¥i', '–Ω–∞—á–µ', '_FILENO', 'roys', 'Í∑º', 'hap', '√•r', '≈Ço', 'aits', ' /*!<']\n",
      "Rank 11: Value vec -> Layer 16, index 9474\n",
      "['fft', '–¥–æ–Ω', ' chorus', 'Courtesy', ':maj', 'emm', 'enz', 'orio', 'itag', 'ultz']\n",
      "Rank 12: Value vec -> Layer 19, index 6041\n",
      "[' damn', ' shit', ' DAM', ' dam', ' fig', 'dam', 'damn', ' Damn', 'rat', ' crap']\n",
      "Rank 13: Value vec -> Layer 19, index 7649\n",
      "[' stuff', ' shit', 'stuff', ' Stuff', 'Stuff', 'shit', '_stuff', ' fuck', ' guy', ' fucking']\n",
      "Rank 14: Value vec -> Layer 23, index 9122\n",
      "['urdu', '„Éº„ÉÜ', 'indr', 'rolling', ' rolling', 'rolled', 'ivery', 'Pressure', 'iere', 'lum']\n",
      "Rank 15: Value vec -> Layer 21, index 11033\n",
      "['–µ–Ω–Ω—ñ', ' Symphony', 'lez', 'atten', 'orts', 'anz', 'hic', ' Morrow', 'oler', 'uario']\n",
      "Rank 16: Value vec -> Layer 27, index 1677\n",
      "['/h', '(h', '-h', ' HI', '.h', ' h', 'HI', ' hi', '\\th', '„Å≤']\n",
      "Rank 17: Value vec -> Layer 16, index 10443\n",
      "['quee', 'ŸàŸÜ€å', 'agu', 'teri', 'cola', ' sm', 'oler', 'alm', 'izione', 'ourt']\n",
      "Rank 18: Value vec -> Layer 18, index 913\n",
      "['amina', ' mi√™u', ' Duel', 'ader', '637', '·ª±', 'fic', 'urement', '_KIND', ' ‰∫¨']\n",
      "Rank 19: Value vec -> Layer 25, index 13329\n",
      "['uger', '.asp', 'ita', 'ÔøΩ', ' favorable', 'ento', ' haul', 'idad', 'utenberg', 'pei']\n",
      "Rank 20: Value vec -> Layer 26, index 12452\n",
      "[' damn', ' damned', ' darn', ' Damn', 'damn', 'Damn', ' fucking', ' bloody', ' dang', ' Fucking']\n"
     ]
    }
   ],
   "source": [
    "# Value vectors similar to embedding difference\n",
    "k = 20\n",
    "norm = model.model.norm  \n",
    "\n",
    "target_vec = toxic_embed - non_toxic_embed\n",
    "dot_prods = torch.einsum(\"nd,d->n\", norm(value_vectors), target_vec)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "\n",
    "print\n",
    "for rank, vec_idx in enumerate(top_value_vecs):\n",
    "    layer = vec_idx.item() // (4096*4)\n",
    "    index = vec_idx.item() % (4096*4)\n",
    "    \n",
    "    print(f\"Rank {rank + 1}: Value vec -> Layer {layer}, index {index}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b33e3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shit', 'crap', 'fuck', ' shit', 'damn', 'Fuck', ' fuck', ' Fuck', ' fucking', ' FUCK']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(target_vec, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b118125",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Value vectors similar to probe\u001b[39;00m\n\u001b[1;32m      2\u001b[0m k \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m\n\u001b[0;32m----> 3\u001b[0m norm \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnorm  \n\u001b[1;32m      5\u001b[0m dot_prods \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mnd,d->n\u001b[39m\u001b[39m\"\u001b[39m, norm(value_vectors), toxic_probe)\n\u001b[1;32m      6\u001b[0m top_value_vecs \u001b[39m=\u001b[39m dot_prods\u001b[39m.\u001b[39mtopk(k, largest\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mindices\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Value vectors similar to probe\n",
    "k = 30\n",
    "norm = model.model.norm  \n",
    "\n",
    "dot_prods = torch.einsum(\"nd,d->n\", norm(value_vectors), toxic_probe)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "\n",
    "for vec_idx in top_value_vecs:\n",
    "    print(f\"Value vec: Layer {vec_idx // (4096*4)}, index {vec_idx % (4096*4)}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fcf645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kommen', ' FUCK', '·ªÉn', 'iyah', 'ÃÜ', 'dirty', 'ÌÑ∏', 'fuck', ' Rudd', ' –ö—Ä–∞']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(toxic_probe, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b762ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_vec, 'llama3_toxic_embed.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f218a5",
   "metadata": {},
   "source": [
    "#### Gemma-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7760fc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2304])\n"
     ]
    }
   ],
   "source": [
    "toxic_probe = torch.load(\"/data/kebl6672/dpo-toxic-general/checkpoints/gemma2_2b_probe.pt\")\n",
    "toxic_probe = toxic_probe.squeeze(0)\n",
    "print(toxic_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f42b484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cc929eef57429dbabe782d477aaf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/gemma-2-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8969f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([239616, 2304])\n"
     ]
    }
   ],
   "source": [
    "# Take the unembedding matrix\n",
    "token_embeds = model.lm_head.weight\n",
    "\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.model.layers[layer_idx].mlp.down_proj.weight.T\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e00d85e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic token IDs: [[34024], [31947], [101886], [48542], [719, 18216]]\n",
      "Non-toxic token IDs: [[17534], [12203], [9141], [44209], [28583]]\n"
     ]
    }
   ],
   "source": [
    "seed_token_toxic = [\"fuck\", \"shit\", \"crap\", \"damn\", \"asshole\"]\n",
    "seed_token_non_toxic = [\"hello\", \"thanks\", \"friend\", \"peace\", \"welcome\"]\n",
    "\n",
    "toxic_token_ids = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "    for tok in seed_token_toxic\n",
    "]\n",
    "\n",
    "non_toxic_token_ids = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "    for tok in seed_token_non_toxic\n",
    "]\n",
    "\n",
    "print(\"Toxic token IDs:\", toxic_token_ids)\n",
    "print(\"Non-toxic token IDs:\", non_toxic_token_ids)\n",
    "\n",
    "toxic_embed = (\n",
    "    torch.stack([\n",
    "        token_embeds[token_ids].mean(dim=0)  # average sub-tokens\n",
    "        for token_ids in toxic_token_ids\n",
    "    ]).mean(dim=0)  # average across seed words\n",
    ")\n",
    "\n",
    "non_toxic_embed = (\n",
    "    torch.stack([\n",
    "        token_embeds[token_ids].mean(dim=0)\n",
    "        for token_ids in non_toxic_token_ids\n",
    "    ]).mean(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dce896aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.model.norm  \n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = torch.einsum(\"vd,d->v\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54377428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 21, index 8804\n",
      "['HSSF', 'sptr', ' umge', ' siihen', '‰æãÂè•', ' advoc', 'Computed', ' riten', 'subpackage', 'glieder']\n",
      "Value vec: Layer 19, index 7297\n",
      "['esModule', 'migrationBuilder', 'celot', ' pinulongan', 'RectangleBorder', 'hoeddwyd', 'oaÃçt', 'WireFormatLite', ' fourrure', 'fillType']\n",
      "Value vec: Layer 19, index 1704\n",
      "['ValueStyle', 'GenerationType', 'BeginContext', 'InjectAttribute', ' –º”ô–∫–∞–ª', 'enumi', 'IntoConstraints', 'AnchorTagHelper', 'ValueGeneration', 'Personensuche']\n",
      "Value vec: Layer 19, index 8366\n",
      "[' dudes', ' dude', ' stuff', ' guys', ' kinda', ' shit', ' guy', ' crap', ' thingy', ' hella']\n",
      "Value vec: Layer 25, index 4751\n",
      "['convertView', 'NavController', 'ClassNotFound', 'cellulose', ' defaultstate', ' Chuk', ' Vikipedi', 'queryInterface', 'ÿØÿßŸÜÿ¥ŸÜÿßŸÖŸáŸî', ' PopupWindow']\n",
      "Value vec: Layer 3, index 4727\n",
      "[' shit', ' Shit', 'shit', 'Shit', ' SHIT', ' crap', ' shits', 'Crap', ' shite', ' shitty']\n",
      "Value vec: Layer 20, index 7196\n",
      "['mybatisplus', ' BoxDecoration', ' AssemblyTitle', ' Sima', 'masing', '–ì–õ–ê', 'wicks', ' Juno', ' Nich', 'GEBURTS']\n",
      "Value vec: Layer 23, index 7107\n",
      "[' fuck', ' fucks', ' fucking', ' fucked', ' shit', 'fuck', 'Fuck', ' Fucking', 'fucking', 'Fucking']\n",
      "Value vec: Layer 11, index 3962\n",
      "[' goddamn', 'fucking', ' fucking', 'FUCK', 'fuck', ' FUCKING', ' Fucking', ' fuckin', ' fucked', 'Fuck']\n",
      "Value vec: Layer 17, index 5484\n",
      "[' poitrine', ' rƒÉ', 'Tob', 'Pon', 'tably', ' voce', ' C√¶sar', 'import', ' cref', 'rop']\n",
      "Value vec: Layer 14, index 1622\n",
      "['ScopeManager', ' akibat', 'expandindo', ' panic', ' mauvaise', ' s√©v', ' seriously', ' adverse', ' inability', 'üò±']\n",
      "Value vec: Layer 4, index 4498\n",
      "[' purpo≈øe', ' An≈ø', ' ≈øever', 'cknow', ' my≈øelf', ' picioare', 'LayoutPanel', ' onAnimation', ' –º”ô–∫–∞–ª', ' manqu√©']\n",
      "Value vec: Layer 2, index 8230\n",
      "[' ngang', 'ggak', ' Pulitzer', ' ŸÖÿ±⁄©', 'ËøôÊù°', ' S√ºd', ' marea', ' P√∫blico', 'atouille', ' p√∫blicos']\n",
      "Value vec: Layer 7, index 3666\n",
      "[' insanely', ' insane', ' absolutely', ' incredibly', ' utterly', ' ridiculous', ' fucking', ' hideous', ' unbelievably', ' shit']\n",
      "Value vec: Layer 5, index 8551\n",
      "['(++', 'ROLS', 'unately', ' (++', 'UTTON', ' Ums', ' incessantly', ' convaincre', 'POUND', 'uffe']\n",
      "Value vec: Layer 4, index 2288\n",
      "[' idiot', ' bastard', ' asshole', ' bastards', ' assholes', ' idiots', ' fucks', 'fucker', ' moron', 'FUCK']\n",
      "Value vec: Layer 6, index 4759\n",
      "[' dudes', ' dude', ' guys', ' guy', ' pissed', ' gotta', ' bloke', ' Dude', 'guys', ' crappy']\n",
      "Value vec: Layer 17, index 2786\n",
      "[' jsPsych', ' fact', 'NUMERIC', 'embley', ' Sph', ' plot', '?>', 'enedig', 'ccion', ' Fes']\n",
      "Value vec: Layer 17, index 3081\n",
      "[' worse', ' worst', ' horrible', 'Worse', ' Worse', ' terrible', ' disastrous', ' Worst', 'Worst', ' horribly']\n",
      "Value vec: Layer 18, index 8171\n",
      "[' Picchu', ' kasarigan', 'Extern√©', ' Hippo', \" '\\\\\\\\;'\", ' Di≈ø', ' Aufs', ' Aufstieg', '≈ÇƒÖd', 'r√©hen']\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "norm = model.model.norm  \n",
    "\n",
    "target_vec = toxic_embed - non_toxic_embed\n",
    "dot_prods = torch.einsum(\"nd,d->n\", norm(value_vectors), target_vec)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "\n",
    "# print(top_value_vecs)\n",
    "\n",
    "for vec_idx in top_value_vecs:\n",
    "    print(f\"Value vec: Layer {vec_idx // (2304*4)}, index {vec_idx % (2304*4)}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "584d41d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shit', ' shit', 'fuck', 'Shit', ' SHIT', ' Shit', ' fuck', 'crap', ' crap', 'Fuck']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(target_vec, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95ce985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Value vec -> Layer 4, index 2288\n",
      "[' idiot', ' bastard', ' asshole', ' bastards', ' assholes', ' idiots', ' fucks', 'fucker', ' moron', 'FUCK']\n",
      "Rank 2: Value vec -> Layer 5, index 5083\n",
      "['amssymb', 'ResponseWriter', ' autorytatywna', '}\".', '://$', ' pa≈Ñstw', 'Tienen', 'p√©die', '√°c', 'Datuak']\n",
      "Rank 3: Value vec -> Layer 7, index 3280\n",
      "[' prochaines', ' nh∆∞', '–ª–∏—è', ' Organ', ' durer', ' crown', ' pengh', '!(\"{}\",', 'organ', 'uidado']\n",
      "Rank 4: Value vec -> Layer 1, index 3962\n",
      "[' ());', '/}.', '--)\\r', '„ÄÇÔºâ', \"']))\\r\", 'AndEndTag', ' }}}', \"']);\\r\", '}\")\\r', \"'},\\r\"]\n",
      "Rank 5: Value vec -> Layer 15, index 3635\n",
      "['D√©c√®s', ' –º”ô–∫–∞–ª', 'MemoryWarning', 'findpost', ' iprot', 'mobileqq', 'Demografia', 'CloseOperation', 'paramref', ']++;']\n",
      "Rank 6: Value vec -> Layer 21, index 9207\n",
      "[' depicted', ' represented', ' portrayed', ' featured', ' pictured', ' profiled', ' interviewed', ' showcased', ' analyzed', ' examined']\n",
      "Rank 7: Value vec -> Layer 3, index 8070\n",
      "['rungsseite', 'posedge', '########.', 'tagHelperRunner', 'expandindo', '+#+#', \" '\\\\\\\\;'\", 'principalTable', 'RUnlock', ' tartalomaj√°nl√≥']\n",
      "Rank 8: Value vec -> Layer 6, index 2476\n",
      "[' similar', ' move', 'HomeController', '√≠slu', 'Êó©„Åè', ' do', 'Move', ' Move', 'SpringBootTest', ' moving']\n",
      "Rank 9: Value vec -> Layer 22, index 4165\n",
      "['·ªÆ', ' anser', ' gus', ' luy', 'enegger', 'Rush', ' Lester', ' Foundry', 'Lester', '‚â∫']\n",
      "Rank 10: Value vec -> Layer 18, index 2597\n",
      "[' yes', ' ridiculous', ' StatelessWidget', ' Yes', ' absurd', 'Yes', ' ludicrous', ' pathetic', 'endwhile', ' awful']\n",
      "Rank 11: Value vec -> Layer 17, index 5506\n",
      "['TagHelper', 'protoimpl', ' ReactDOM', ' typelib', 'ArrowToggle', '#+#', ' –ø—Ä–µ–ø—Ä–∞—Ç–∫–∏', ' gainera', ' –ú–µ–∫—Å–∏—á–∫–∞', 'TypeConverter']\n",
      "Rank 12: Value vec -> Layer 5, index 4757\n",
      "[' EconPapers', 'AndEndTag', 'AddHtmlAttribute', 'InSection', ' ¬©Ô∏è', 'ViewImports', '\\ufeff#', 'TagMode', '–≥”Ä', 'ŸÇÿß€åŸÜÿßŸÇŸÑÿßÿ±']\n",
      "Rank 13: Value vec -> Layer 7, index 9157\n",
      "['########.', ' initWithFrame', ' clogging', ' –¥–æ–ø–∏—Å–∞–≤—à–∏', ' cleanliness', 'WebServlet', '’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä', 'Â≠ò‰∫é‰∫íËÅîÁΩëÊ°£Ê°àÈ¶Ü', '__;', ' fouling']\n",
      "Rank 14: Value vec -> Layer 16, index 1468\n",
      "['OGND', ' GetEnumerator', ' contextLoads', '\\ufeff//', ' ÿßŸÑÿØŸàŸÑŸâ', 'Geplaatst', 'Personensuche', ' getchar', ' Bourgoin', ' ŸÑŸÑŸÖÿπÿßÿ±ŸÅ']\n",
      "Rank 15: Value vec -> Layer 1, index 8886\n",
      "[' my≈øelf', ' it≈øelf', ' Efq', ' Monfieur', ' The≈øe', ' them≈øelves', ' Majefty', ' Jefus', ' An≈ø', '≈øelf']\n",
      "Rank 16: Value vec -> Layer 18, index 2902\n",
      "[' String', 'toString', 'String', 'evos', ' string', 'str', 'ToString', ' str', ' }^{[', ' Strings']\n",
      "Rank 17: Value vec -> Layer 21, index 5661\n",
      "[' particular', ' certain', 'particular', ' specific', 'certain', ' Particular', ' Certain', 'Certain', ' PARTICULAR', 'Particular']\n",
      "Rank 18: Value vec -> Layer 1, index 8498\n",
      "[' betweenstory', 'HasAnnotation', ' ligiloj', 'TintMode', ' Exactos', 'ÿ±ÿ¥ŸÅ', ' Administrativna', 'lankton', ' ProtoMessage', '—å–∞–∂']\n",
      "Rank 19: Value vec -> Layer 3, index 8742\n",
      "[' Ÿà€å⁄©€å\\u200cŸæÿØ€åÿß€å', 'fjspx', '√®dia', '\".\\r', 'ueuse', 'EnableWeb', \"]-'\", '...\\r', ' amorosa', ' ***/']\n",
      "Rank 20: Value vec -> Layer 6, index 3007\n",
      "['–ò—Å–∫”ô—Ä–º”ô–ª”ô—Ä', ' VICE', 'bootstrapcdn', ' tumblr', 'ngdoc', 'i√ßos', ' estim√©s', 'saraba', 'GTCX', ' Polyp']\n",
      "Rank 21: Value vec -> Layer 3, index 3261\n",
      "['IntoConstraints', 'mybatisplus', 'sidemargin', 'AlterField', 'fromnode', '\\ufeff#', 'IsMutable', 'StringCopy', 'AddHtmlAttribute', '·ªën']\n",
      "Rank 22: Value vec -> Layer 3, index 4727\n",
      "[' shit', ' Shit', 'shit', 'Shit', ' SHIT', ' crap', ' shits', 'Crap', ' shite', ' shitty']\n",
      "Rank 23: Value vec -> Layer 15, index 3003\n",
      "['AndEndTag', 'protoimpl', ' maravilloso', 'enumii', 'StoreMessageInfo', '‡∞≤‡∞æ‡∞≤‡±Å', ' F√º√üen', ' medlemmer', ' ritratto', ' ÿßÿ≥ÿ™ŸÜÿßÿØŸâ']\n",
      "Rank 24: Value vec -> Layer 17, index 7018\n",
      "[' area', ' region', ' era', ' period', 'area', ' Ÿäÿ™ŸäŸÖŸá', ' r√©gion', ' situation', ' regi√£o', 'Êó∂‰ª£']\n",
      "Rank 25: Value vec -> Layer 15, index 8094\n",
      "['‚ìò', 'ServletRequest', ' colar', ' cucharadas', 'foreignKey', 'oOo', ' Fla', 'IMIENTO', ' \"', '√ì']\n",
      "Rank 26: Value vec -> Layer 17, index 6911\n",
      "[' replaced', 'replaced', ' replace', ' replacement', ' Replaced', ' Replace', ' Replacement', 'replacement', 'Replace', ' replaces']\n",
      "Rank 27: Value vec -> Layer 9, index 2351\n",
      "[' ÿµŸàÿ™ŸäŸá', 'ArrowToggle', ']\")]', 'ÂèëË°®‰∫é', ' Ÿà€å⁄©€å\\u200cŸæÿØ€åÿß', ' ActionResult', 'WriteLiteral', ' endwhile', 'DockStyle', 'TabIndex']\n",
      "Rank 28: Value vec -> Layer 24, index 7991\n",
      "['anti', 'casian', ' anti', 'findpost', 'tagHelperRunner', 'er', 'worthiness', ' woning', 'Pio', 'APORE']\n",
      "Rank 29: Value vec -> Layer 10, index 7039\n",
      "['FailureListener', ' onCancelled', ' NSError', ' onFailure', 'saraba', ' waste', ' ato', ' insecure', ' ÿßŸÑŸàÿ∑ŸÜŸäŸá', ' strto']\n",
      "Rank 30: Value vec -> Layer 9, index 6593\n",
      "['findpost', 'principalTable', 'izr', 'IBOutlet', 'AlterField', ' >=\",', '>//', 'offsetof', 'sizeCache', 'ficie']\n",
      "Rank 31: Value vec -> Layer 4, index 1804\n",
      "['Clik', 'RenderAtEndOf', 'ruptedException', 'GEBURTSDATUM', \" __('\", ' ‚Äò', '\"\"\"', ' jLabel', \" '\", '–Ω–æ–º–∞']\n",
      "Rank 32: Value vec -> Layer 23, index 7586\n",
      "['+:+', 'rungsseite', ' ŸÑŸÑŸÖÿπÿßÿ±ŸÅ', 'AndEndTag', 'ocino', ' juveniles', ' docent', '</thead>', ' Ÿà€å⁄©€å\\u200cŸæÿØ€åÿß€å', ' salaried']\n",
      "Rank 33: Value vec -> Layer 4, index 5518\n",
      "[' rich', ' riche', ' Wealth', ' wealth', 'wealth', ' richest', ' wealthy', 'rich', 'Rich', ' riches']\n",
      "Rank 34: Value vec -> Layer 23, index 675\n",
      "['Density', 'MEMORANDUM', 'kpi', ' Density', 'chong', ' actionMode', 'CppCodeGen', 'DropColumn', 'uxxxx', ' novios']\n",
      "Rank 35: Value vec -> Layer 15, index 2734\n",
      "[' R√©ponses', 'FormTagHelper', 'ErrUnexpectedEOF', 'LabelTagHelper', 'StateToProps', ' ExecuteAsync', '—Å—ã–ª–∫—ñ', '√©valuateur', ' AppColors', 'migrationBuilder']\n",
      "Rank 36: Value vec -> Layer 25, index 8495\n",
      "[' parts', 'parts', ' PARTS', ' Parts', 'Parts', 'PARTS', ' pieces', ' Pieces', ' bits', ' Teile']\n",
      "Rank 37: Value vec -> Layer 2, index 3129\n",
      "[' withRouter', 'doctype', 'enumi', 'SourceChecksum', ' resourceCulture', ' Majest√©', 'DOCTYPE', 'Ë´áÁ§æ', ' \"<?', 'Personendaten']\n",
      "Rank 38: Value vec -> Layer 16, index 2944\n",
      "['GEBURTSDATUM', '–ë–∏–ª–≥–∞–ª–¥–∞—Ö–∞—Ä—à', '__*/', 'AndEndTag', ' nahilalakip', ' CreateTagHelper', \" '\\\\\\\\;'\", ' –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–∞', ' Ÿà€å⁄©€å\\u200cŸæÿØ€å', 'CppMethod']\n",
      "Rank 39: Value vec -> Layer 11, index 8015\n",
      "['tagHelperRunner', ' betweenstory', ' ModelExpression', 'AddTagHelper', ' defaultstate', ' SafeMath', 'uxxxx', ' propOrder', 'IUrlHelper', 'TagMode']\n",
      "Rank 40: Value vec -> Layer 9, index 897\n",
      "['NameInMap', 'webElementXpaths', 'complexType', ' CreateTagHelper', '})`', \"'][$\", ' endforeach', 'jsxFileName', 'MemoryWarning', 'kuit']\n",
      "Rank 41: Value vec -> Layer 4, index 1619\n",
      "['########.', 'toir', 'titleMargin', 'mobileqq', 'ÿµÿØÿßÿ±', ' h√¢te', ' nakalista', 'CardContent', ' OnInit', ' Wikim√©dia']\n",
      "Rank 42: Value vec -> Layer 16, index 9029\n",
      "[' –æ—Ä–∏–≥—ñ–Ω–∞–ª—É', 'awtextra', ' ÿØ€å⁄©⁄æ€å€í', 'MigrationBuilder', ' ŸÑŸäŸÜŸÉ', ' commission', 'BeforeMethod', ' InputDecoration', ' dessus', 'styleable']\n",
      "Rank 43: Value vec -> Layer 22, index 4810\n",
      "[' after', 'after', ' After', ' aft', 'After', ' –ø–æ—Å–ª–µ', ' AFTER', ' efter', ' ÿ®ÿπÿØ', ' nach']\n",
      "Rank 44: Value vec -> Layer 25, index 4129\n",
      "['providedIn', ' AssemblyCulture', 'AnchorTagHelper', 'hlung', '<?>>', 'segno', 'caloosa', 'Âºà', ' studio', ' studios']\n",
      "Rank 45: Value vec -> Layer 15, index 2072\n",
      "['AZIONE', 'cussion', 'abatic', 'Barton', 'goin', ' Hano', 'jaan', 'arithmic', 'œÅŒØŒµœÇ', ' Sita']\n",
      "Rank 46: Value vec -> Layer 22, index 9162\n",
      "[' autorytatywna', '+:+', 'EndGlobalSection', 'webElementXpaths', 'PMailer', 'AutoScaleMode', 'GenerationType', '(!__', 'WriteTagHelper', 'ResumeLayout']\n",
      "Rank 47: Value vec -> Layer 12, index 1218\n",
      "[' EconPapers', ' resourceCulture', 'ResumeLayout', 'IUrlHelper', '‘æ’°’∂’∏’©', 'titudine', 'IfNeeded', 'DispatchToProps', 'PUTY', 'Extern√©']\n",
      "Rank 48: Value vec -> Layer 3, index 7175\n",
      "['ener', 'ment', 'tic', ' plati', 'Mientras', ' CURIAM', ' Mientras', '\">&', 'while', 'haa']\n",
      "Rank 49: Value vec -> Layer 11, index 4228\n",
      "['-------\\x0c', 'MessageTagHelper', ' autorytatywna', '‰∏∫‰æã', \"::_('\", 'setAccessible', ' combineReducers', 'cydow', 'actionMode', 'Caution']\n",
      "Rank 50: Value vec -> Layer 10, index 3875\n",
      "['@', '<blockquote>', 'co', 'gettext', ' au', 'hv', '¬∫', ' tri', ' calloc', ' ƒÉn']\n"
     ]
    }
   ],
   "source": [
    "# Value vectors similar to probe\n",
    "k = 50\n",
    "norm = model.model.norm  \n",
    "\n",
    "dot_prods = torch.einsum(\"nd,d->n\", norm(value_vectors), toxic_probe)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "top_scores = dot_prods[top_value_vecs]\n",
    "\n",
    "print\n",
    "for rank, vec_idx in enumerate(top_value_vecs):\n",
    "    layer = vec_idx.item() // (2304*4)\n",
    "    index = vec_idx.item() % (2304*4)\n",
    "    \n",
    "    print(f\"Rank {rank + 1}: Value vec -> Layer {layer}, index {index}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60d8b6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rungsseite', 'fuck', 'Fucking', 'ÿßŸÜŸäŸÅ', 'AndEndTag', 'expandindo', ' asshole', ' Fuck', ' fucking', 'IntoConstraints']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(toxic_probe, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55633fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_vec, 'gemma2_toxic_embed.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7a227",
   "metadata": {},
   "source": [
    "#### Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0cabdba",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/kebl6672/dpo-toxic-general/checkpoints/mistral_probe.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m toxic_probe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m/data/kebl6672/dpo-toxic-general/checkpoints/mistral_probe.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m toxic_probe \u001b[39m=\u001b[39m toxic_probe\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(toxic_probe\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/data/kebl6672/miniconda3/envs/toxic/lib/python3.12/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/data/kebl6672/miniconda3/envs/toxic/lib/python3.12/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    446\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/data/kebl6672/miniconda3/envs/toxic/lib/python3.12/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/kebl6672/dpo-toxic-general/checkpoints/mistral_probe.pt'"
     ]
    }
   ],
   "source": [
    "toxic_probe = torch.load(\"/data/kebl6672/dpo-toxic-general/checkpoints/mistral_probe.pt\")\n",
    "toxic_probe = toxic_probe.squeeze(0)\n",
    "print(toxic_probe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d98b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85468d6a7a3b48da8364e9306eb5ff24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b6ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([458752, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Take the unembedding matrix\n",
    "token_embeds = model.lm_head.weight\n",
    "\n",
    "value_vectors = torch.cat(\n",
    "    [\n",
    "        model.model.layers[layer_idx].mlp.down_proj.weight.T\n",
    "        for layer_idx in range(model.config.num_hidden_layers)\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "print(value_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc2861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic token IDs: [[4159], [5492], [21849], [9741], [25676]]\n",
      "Non-toxic token IDs: [[6312, 28709], [8196], [1832], [6405], [10058]]\n"
     ]
    }
   ],
   "source": [
    "seed_token_toxic = [\"fuck\", \"shit\", \"crap\", \"damn\", \"asshole\"]\n",
    "seed_token_non_toxic = [\"hello\", \"thanks\", \"friend\", \"peace\", \"welcome\"]\n",
    "\n",
    "toxic_token_ids = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "    for tok in seed_token_toxic\n",
    "]\n",
    "\n",
    "non_toxic_token_ids = [\n",
    "    tokenizer(tok, add_special_tokens=False)[\"input_ids\"]\n",
    "    for tok in seed_token_non_toxic\n",
    "]\n",
    "\n",
    "print(\"Toxic token IDs:\", toxic_token_ids)\n",
    "print(\"Non-toxic token IDs:\", non_toxic_token_ids)\n",
    "\n",
    "toxic_embed = (\n",
    "    torch.stack([\n",
    "        token_embeds[token_ids].mean(dim=0)  # average sub-tokens\n",
    "        for token_ids in toxic_token_ids\n",
    "    ]).mean(dim=0)  # average across seed words\n",
    ")\n",
    "\n",
    "non_toxic_embed = (\n",
    "    torch.stack([\n",
    "        token_embeds[token_ids].mean(dim=0)\n",
    "        for token_ids in non_toxic_token_ids\n",
    "    ]).mean(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda670d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unembed_to_text(vector, model, tokenizer, k=10):\n",
    "    norm = model.model.norm  \n",
    "    lm_head = model.lm_head.weight\n",
    "    dots = torch.einsum(\"vd,d->v\", lm_head, norm(vector))\n",
    "    top_k = dots.topk(k).indices\n",
    "    return tokenizer.batch_decode(top_k, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9714355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value vec: Layer 22, index 1061\n",
      "['fuck', 'fucking', 'Fuck', 'fucked', 'shit', 'shit', 'bullshit', 'asshole', 'shitty', 'assh']\n",
      "Value vec: Layer 22, index 15900\n",
      "['screw', 'fuck', 'Fuck', 'fucked', 'Scre', 'fucking', 'shit', 'shit', 'bullshit', 'piss']\n",
      "Value vec: Layer 17, index 6981\n",
      "['damn', 'damned', 'fucking', 'bloody', 'freak', 'god', 'dam', 'Fuck', 'Dam', 'Dam']\n",
      "Value vec: Layer 19, index 4689\n",
      "['crap', 'shit', 'damn', 'shit', 'damned', 'hell', 'bitch', 'piss', 'Hell', 'fuck']\n",
      "Value vec: Layer 23, index 12879\n",
      "['freak', 'fucking', 'fr', 'damn', 'damned', 'Fuck', 'bloody', 'god', 'fuck', 'eff']\n",
      "Value vec: Layer 19, index 6318\n",
      "['dude', 'kinda', 'crap', 'shit', 'gotta', 'freak', 'ain', 'guy', 'guys', 'awesome']\n",
      "Value vec: Layer 22, index 5047\n",
      "['shit', '****', 'shit', '***', '**', 'fucking', 'Fuck', '******', '***', '**']\n",
      "Value vec: Layer 22, index 9629\n",
      "['squ', 'jack', 'shit', 'crap', 'Jack', 'shit', 'dick', 'jack', 'zip', 'Jack']\n",
      "Value vec: Layer 25, index 8070\n",
      "['reen', 'furt', 'ague', 'sha', 'chaft', 'v√≠', 'ÔøΩ', 'Za', 'dale', 'emi']\n",
      "Value vec: Layer 18, index 10976\n",
      "['oen', 'ysk', 'enth', '–≥–∞–Ω', '/******/', '»õi', 'alf', 'Sever', 'alias', 'consent']\n",
      "Value vec: Layer 17, index 8050\n",
      "['sex', 'sexual', 'penis', 'Sex', 'porn', 'sex', 'mast', 'dick', 'fuck', 'Fuck']\n",
      "Value vec: Layer 27, index 14468\n",
      "['/******/', 'vscale', 'AtA', 'qpoint', 'ICENSE', 'Geplaatst', '‰∏∂', 'opyright', 'CLUD', 'BPACK']\n",
      "Value vec: Layer 19, index 9117\n",
      "['damn', 'damned', 'dam', 'Dam', 'Dam', 'pes', 'dam', 'stupid', 'bloody', 'fucking']\n",
      "Value vec: Layer 18, index 2787\n",
      "['stupid', 'crap', 'nonsense', 'bullshit', 'assh', 'shit', 'foolish', 'idiot', 'garbage', 'asshole']\n",
      "Value vec: Layer 27, index 13909\n",
      "['/******/', 'vscale', 'qpoint', 'AtA', 'ICENSE', 'Geplaatst', 'listade', '‰∏∂', '–°—Ç–∞–Ω–æ–≤', '][<']\n",
      "Value vec: Layer 27, index 8547\n",
      "['/******/', 'Geplaatst', 'vscale', 'AtA', 'qpoint', 'ICENSE', 'listade', 'CLUD', '‰∏∂', 'BPACK']\n",
      "Value vec: Layer 22, index 9770\n",
      "['/*!', 'atte', 'ÔøΩ', '—Å–æ–±', 'ahan', 'ierto', 'LT', '–∑–∞–Ω', 'chaft', 'Ïû•']\n",
      "Value vec: Layer 26, index 10726\n",
      "['alis', 'ieg', 'amos', '—á–∞', 'gebra', 'agine', 'virt', 'eing', 'iw', 'geon']\n",
      "Value vec: Layer 27, index 10147\n",
      "['foul', 'corrupt', 'mal', 'dirty', 'fraud', 'abuse', 'corruption', 'dirty', 'evil', 'poison']\n",
      "Value vec: Layer 14, index 2292\n",
      "['shit', 'crap', 'damn', 'fucking', 'shit', 'fuck', 'shitty', 'bullshit', 'stupid', 'sucks']\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "norm = model.model.norm  \n",
    "\n",
    "target_vec = toxic_embed - non_toxic_embed\n",
    "dot_prods = torch.einsum(\"nd,d->n\", norm(value_vectors), target_vec)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "\n",
    "for vec_idx in top_value_vecs:\n",
    "    print(f\"Value vec: Layer {vec_idx // (4096*4)}, index {vec_idx % (4096*4)}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f76ff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shit', 'crap', 'fuck', 'shit', 'fucking', 'Fuck', 'fucked', 'asshole', 'shitty', 'bullshit']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(target_vec, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_vec, 'mistral_toxic_embed.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af164f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Value vec -> Layer 22, index 1061\n",
      "['fuck', 'fucking', 'Fuck', 'fucked', 'shit', 'shit', 'bullshit', 'asshole', 'shitty', 'assh']\n",
      "Rank 2: Value vec -> Layer 2, index 8896\n",
      "['jav', 'olic', 'uler', 'witness', 'cav', '@@', 'pez', 'ÔøΩ', 'idiot', 'bear']\n",
      "Rank 3: Value vec -> Layer 14, index 2292\n",
      "['shit', 'crap', 'damn', 'fucking', 'shit', 'fuck', 'shitty', 'bullshit', 'stupid', 'sucks']\n",
      "Rank 4: Value vec -> Layer 15, index 2454\n",
      "['fucking', 'bullshit', 'stupid', 'shit', 'crap', 'piss', 'disgust', 'ridiculous', 'fucked', 'fuck']\n",
      "Rank 5: Value vec -> Layer 1, index 9939\n",
      "['atti', 'stein', 'adu', '—é', 'ess', 'iele', 'blur', 'auge', 'erg', '–∏–º']\n",
      "Rank 6: Value vec -> Layer 13, index 13888\n",
      "['Wind', 'Mal', 'esh', 'Sto', 'ulo', 'Mono', 'Lower', 'moy', 'winds', 'nomin']\n",
      "Rank 7: Value vec -> Layer 3, index 11985\n",
      "['NU', 'ower', 'iaz', 'heck', 'aggi', 'ust', 'abstract', 'arroll', 'essen', 'dm']\n",
      "Rank 8: Value vec -> Layer 22, index 5047\n",
      "['shit', '****', 'shit', '***', '**', 'fucking', 'Fuck', '******', '***', '**']\n",
      "Rank 9: Value vec -> Layer 11, index 3134\n",
      "['Holl', 'Jur', 'zek', 'shr', 'tempor', 'ells', 'mast', 'chnitt', 'quare', 'Required']\n",
      "Rank 10: Value vec -> Layer 4, index 1609\n",
      "['nonsense', 'empty', 'driv', 'Tut', 'refresh', 'virtual', 'waste', 'bid', '‚Ä¶', '!']\n",
      "Rank 11: Value vec -> Layer 22, index 3443\n",
      "['DER', '·ª≥', 'ionato', 'Excell', 'icz', 'ERNAL', 'hythm', 'ocz', 'irtual', 'igd']\n",
      "Rank 12: Value vec -> Layer 25, index 3529\n",
      "['ne', 'Ne', 'Ne', 'neo', 'ne', 'n√©', 'Neil', '–ù–µ', 'white', 'neur']\n",
      "Rank 13: Value vec -> Layer 10, index 10581\n",
      "['sources', 'sources', 'source', '≈Ü', '√©ra', 'Sources', 'Mot', 'zd', 'spr', 'Source']\n",
      "Rank 14: Value vec -> Layer 27, index 15268\n",
      "['–Ω—è', '–ª—è', '—è', '—á–∞', '–∂–∞', 'nia', '—Ä—è', '—à–∞', 'ja', '—Ç—è']\n",
      "Rank 15: Value vec -> Layer 4, index 7710\n",
      "['inition', 'capacity', 'oko', 'ivas', 'urance', 'orney', 'ERCHANT', 'u√ü', 'yside', 'proceed']\n",
      "Rank 16: Value vec -> Layer 23, index 6313\n",
      "['web', 'web', 'Web', 'Web', 'webs', 'graphic', 'webpack', 'Website', 'Graph', 'website']\n",
      "Rank 17: Value vec -> Layer 20, index 1349\n",
      "['ass', 'fool', 'ASS', 'ass', 'idiot', 'ASS', 'Ass', 'complete', 'hole', 'Ass']\n",
      "Rank 18: Value vec -> Layer 0, index 14580\n",
      "['entr', 'aye', '√≥j', 'ioc', 'ÁóÖ', 'tainment', 'Sach', 'ÔøΩ', 'NC', 'osto']\n",
      "Rank 19: Value vec -> Layer 18, index 5446\n",
      "['/******/', 'Gon', 'gon', 'cultiv', 'Rodr', 'Wil', 'Gar', 'Cameron', 'pur', 'dull']\n",
      "Rank 20: Value vec -> Layer 2, index 582\n",
      "['illet', 'sle', 'criptor', 'alph', 'thon', 'Louise', 'Pict', 'intellect', 'RC', 'ategory']\n",
      "Rank 21: Value vec -> Layer 5, index 15106\n",
      "['aris', 'inois', 'zas', 'isz', 'themselves', 'scal', 's', 'ars', '‚ÇÅ', 'ascript']\n",
      "Rank 22: Value vec -> Layer 2, index 8522\n",
      "['bras', 'ijk', 'bara', 'IMIT', 'memor', '–∞–Ω', 'elsewhere', 'snd', 'dedic', 'feld']\n",
      "Rank 23: Value vec -> Layer 27, index 5067\n",
      "['off', 'ass', 'off', 'ASS', 'ingen', 'ons', 'ija', 'Off', 'oko', 'offs']\n",
      "Rank 24: Value vec -> Layer 27, index 1424\n",
      "['Co', 'Jew', 'Wood', 'Cl', 'Flor', 'Crown', 'Mark', 'Br', 'Clay', 'Curt']\n",
      "Rank 25: Value vec -> Layer 26, index 10193\n",
      "['web', 'web', 'Web', 'Web', 'webs', 'webpack', 'WE', 'Á∂≤', 'EB', 'internet']\n",
      "Rank 26: Value vec -> Layer 1, index 5845\n",
      "['achuset', 'ÔøΩ', 'worn', 'wo', 'oard', 'Eins', 'moder', 'aye', '√§st', 'ARK']\n",
      "Rank 27: Value vec -> Layer 15, index 2086\n",
      "['getElementById', 'rale', 'glow', 'EO', 'reen', 'Mach', 'ect', 'jos', 'emu', 'mach']\n",
      "Rank 28: Value vec -> Layer 12, index 9435\n",
      "['elij', 'lte', 'mv', 'Rein', 'gra', 'izi', 'adesh', 'msm', 'ken', 'Station']\n",
      "Rank 29: Value vec -> Layer 27, index 7944\n",
      "['elf', 'touch', 'ioso', 'touch', 'gorith', 'ugno', 'Touch', 'iot', 'ios', 'quet']\n",
      "Rank 30: Value vec -> Layer 26, index 3061\n",
      "['mor', 'env', 'env', 'vag', 'imply', 'appearing', 'mont', 'associations', 'iles', 'respectively']\n",
      "Rank 31: Value vec -> Layer 20, index 1120\n",
      "['adu', 'resid', 'sec', 'Mam', 'ExecutionContext', 'aggi', 'oton', 'seg', 'Lad', 'intim']\n",
      "Rank 32: Value vec -> Layer 5, index 12741\n",
      "['tou', 'pun', 'f√∂r', 'synchronized', 'SG', 'alia', 'Images', 'zoom', 'Labels', 'vb']\n",
      "Rank 33: Value vec -> Layer 13, index 6053\n",
      "['aude', 'Tol', 'aw', 'caused', 'ASS', 'awi', 'auf', 'ond', 'MAGES', 'climb']\n",
      "Rank 34: Value vec -> Layer 14, index 11281\n",
      "['sexual', 'sex', 'sex', 'girls', 'women', 'Sex', 'dating', 'dating', 'porn', 'sexy']\n",
      "Rank 35: Value vec -> Layer 22, index 12672\n",
      "['Mer', 'Mer', 'Lem', 'Way', 'Merge', 'Will', 'Ralph', 'omer', 'lio', 'mer']\n",
      "Rank 36: Value vec -> Layer 23, index 131\n",
      "['if', 'if', 'IF', 'IF', 'If', 'If', 'wenn', 'ifs', 'Â¶Ç', 'ife']\n",
      "Rank 37: Value vec -> Layer 8, index 2789\n",
      "['bler', 'inder', 'itel', 'Hugo', 'azi', 'itzer', 'puzzle', 'XY', 'gravity', 'filer']\n",
      "Rank 38: Value vec -> Layer 2, index 13027\n",
      "['√•n', 'dern', 'plaat', '—Ä—É–∫–æ', 'Lis', 'eken', 'etto', 'Norway', 'Fernando', 'gon']\n",
      "Rank 39: Value vec -> Layer 17, index 3883\n",
      "['anto', 'ester', 'oda', 'idal', 'Aires', 'ools', 'ville', 'atos', 'inta', 'CH']\n",
      "Rank 40: Value vec -> Layer 21, index 4730\n",
      "['grain', 'Rice', 'rice', 'osto', 'eken', 'rice', 'bare', 'ionato', 'odox', 'Spot']\n",
      "Rank 41: Value vec -> Layer 4, index 13678\n",
      "['inode', 'hall', 'Pow', 'r√§', 'hall', 'land', 'uen', 'hollow', 'ilis', 'plaat']\n",
      "Rank 42: Value vec -> Layer 18, index 6743\n",
      "['rede', 'units', 'units', 'unit', 'unit', 'izo', 'Unit', 'fund', 'share', 'share']\n",
      "Rank 43: Value vec -> Layer 19, index 9245\n",
      "['behind', 'Behind', 'backend', 'backend', 'rear', 'bg', 'bg', 'Backend', 'ËÉå', 'hind']\n",
      "Rank 44: Value vec -> Layer 23, index 3668\n",
      "['association', 'associate', 'associations', 'Associ', 'associ', 'associ', 'Associ', 'Association', 'associated', 'affili']\n",
      "Rank 45: Value vec -> Layer 19, index 4689\n",
      "['crap', 'shit', 'damn', 'shit', 'damned', 'hell', 'bitch', 'piss', 'Hell', 'fuck']\n",
      "Rank 46: Value vec -> Layer 27, index 5053\n",
      "['inton', 'ng', 'mid', 'erm', 'saf', 'erst', 'pa', 'child', 'lon', 'ele']\n",
      "Rank 47: Value vec -> Layer 10, index 6660\n",
      "['Êéâ', 'yet', 'aise', 'utch', 'ÊÅØ', 'Yet', 'ura', 'ASS', 'Ê≠¢', 'limits']\n",
      "Rank 48: Value vec -> Layer 6, index 7364\n",
      "['lie', 'fuck', 'da', 'Fuck', 'foul', 'bitch', 'crap', 'lie', 'nuts', 'tolerance']\n",
      "Rank 49: Value vec -> Layer 16, index 4861\n",
      "['uple', 'addy', '‡∏ì', 'dsi', '–¥–∞–Ω', 'asant', 'inth', 'gepublice', 'agh', 'mma']\n",
      "Rank 50: Value vec -> Layer 1, index 13026\n",
      "['atin', 'uper', 'cke', 'naked', 'alter', 'tocol', 'Daddy', 'exh', 'NF', 'ÂÄç']\n"
     ]
    }
   ],
   "source": [
    "# Value vectors similar to probe\n",
    "k = 50\n",
    "norm = model.model.norm  \n",
    "\n",
    "dot_prods = torch.einsum(\"nd,d->n\", norm(value_vectors), toxic_probe)\n",
    "top_value_vecs = dot_prods.topk(k).indices\n",
    "top_scores = dot_prods[top_value_vecs]\n",
    "\n",
    "for rank, vec_idx in enumerate(top_value_vecs):\n",
    "    layer = vec_idx.item() // (4096*4)\n",
    "    index = vec_idx.item() % (4096*4)\n",
    "    \n",
    "    print(f\"Rank {rank + 1}: Value vec -> Layer {layer}, index {index}\")\n",
    "    print(unembed_to_text(value_vectors[vec_idx], model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "070f31f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shit', 'shit', 'fuck', 'Fuck', 'fucking', 'fucked', 'assh', 'asshole', 'upid', 'bullshit']\n"
     ]
    }
   ],
   "source": [
    "print(unembed_to_text(toxic_probe, model, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
